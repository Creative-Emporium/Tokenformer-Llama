# Model Parameters
d_model: 64               # Embedding dimension
n_layers: 4               # Number of TokenFormer blocks
gqa_num_groups: 2         # Number of groups for GQA
expansion_factor: 4       # Expansion factor for FFN
dropout: 0.1              # Dropout probability
max_seq_len: 128          # Maximum sequence length
init_model_size: 1000000  # Initial target size for model params
scale_model: false        # If to scale the model via initialization and scale_factor 
scale_factor: 1           # Factor for scaling the model
norm_activation_type: "l2_norm_gelu" # whether to use l2 norm and gelu after the linear layer. options: gelu, relu, swish, l2_norm_gelu
init_method: "normal" # weight initialization method for all modules except the output projection. Options: normal, xavier_uniform
output_layer_init_method: "wang_init" #output projection weight initialization. Options: normal, wang_init
use_flash_attention: false # whether to use flash attention
rotary_pct: 0.25 # rotary embeddings dimension as a percentage of d_model
use_bias_in_attn_linear: false # whether to use biases in the linear projection layers

# Training Parameters
batch_size: 32            # Batch size
learning_rate: 0.0001     # Learning rate
weight_decay: 0.0001      # Weight decay
num_epochs: 3             # Number of training epochs
gradient_accumulation_steps: 5 # Gradient accumulation steps
gradient_clipping: 1.0 # value to clip gradient norm at. If false then no clipping is applied
warmup_steps: 100
lr_scheduler: "cosine" # linear or cosine scheduling options
optimizer: "adamw" # optimizer choice between adam, adan and lion
train_ratio: 0.9

# Dataset Parameters
local_dataset: false    # Flag if using local dataset
data_path: "tinyshakespeare.txt" # Path to local dataset, will be used if local_dataset is true
online_dataset: true # Flag if using online dataset
dataset_name: "wikitext" # Huggingface dataset
dataset_config: "wikitext-2-raw-v1" # Huggingface dataset config

# Saving Parameters
save_path: "trained_model"
